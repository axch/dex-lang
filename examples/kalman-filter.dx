' # A Kalman filter in Dex

' Based on [TFP's Kalman filter](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/distributions/linear_gaussian_ssm.py), written by Dave Moore and others.

import linalg

def LogProb : Type = Float
def Vec (n:Type)          : Type = n=>Float
def Mat (n:Type) (m:Type) : Type = n=>m=>Float
def SqMat (n:Type)        : Type = n=>n=>Float

' ## Multivariate Gaussian distribtions

' The first object of interest is the multivariate Gaussian distribution.
We index the dimensions of the Gaussian by the type parameter `n`.

def Gaussian (n:Type)     : Type =
  { mean       : Vec n
  & covariance : SqMat n
  & precision  : SqMat n
  & logDetCov  : Float }

def makeGaussianFromPrecision (mean:Vec n) (precision: SqMat n) : Gaussian n =
  covariance = invert precision
  {   mean = mean
    , covariance = covariance
    , precision  = precision
    , logDetCov = snd $ sign_and_log_determinant covariance }

def makeGaussianFromCovariance (mean:Vec n) (covariance: SqMat n) : Gaussian n =
  precision = invert covariance
  {   mean = mean
    , covariance = covariance
    , precision  = precision
    , logDetCov = snd $ sign_and_log_determinant covariance }

def logDensity (g:Gaussian n) (x:Vec n) : LogProb =
   d = x - get_at #mean g
   normalizer = - (i_to_f (size n) / 2.0) * log (2.0 * pi) - 0.5 * get_at #logDetCov g
   -0.5 * (d `vdot` (get_at #precision g **. d)) + normalizer

def scaleGaussian (scaleMat:Mat o i) (g:Gaussian i) : Gaussian o =
  newMean = scaleMat **. get_at#mean g
  newCov = (scaleMat ** get_at#covariance g) ** transpose scaleMat
  makeGaussianFromCovariance newMean newCov

def convolveGaussians (g1:Gaussian n) (g2:Gaussian n) : Gaussian n =
  newMean = get_at#mean g1 + get_at#mean g2
  newCov  = get_at#covariance g1 + get_at#covariance g2
  makeGaussianFromCovariance newMean newCov

myGaussian = makeGaussianFromPrecision [2.0, 2.0] [[1.0, 0.0],
                                                   [0.0, 2.0]]
myGaussian2 = makeGaussianFromPrecision [2.0, 2.0] [[1.0, 0.2],
                                                    [0.2, 2.0]]

logDensity myGaussian [1.0, 2.0]
> -1.991303

logDensity myGaussian2 [1.0, 2.0]
> -2.001405

scaleGaussian (2.0 .* eye) myGaussian

' ## Gaussian transitions

' The second object of interest is the Gaussian transition: given a
Gaussian on input space `i`, a linear transformation from `i` to `o`,
and Gaussian noise on `o`, the induced joint distribution on `(i, o)`
is also an analytic Gaussian, as are its marginal and conditional
distributions.  These transitions can serve as either a
linear-Gaussian transition or a linear-Gaussian observation in a state
space model.

def GaussianTransition (i:Type) (o:Type) : Type =
 { transitionMat : Mat o i
 & transitionNoise : Gaussian o }

' `applyTransition` computes the marginal Gaussian on the output space `o`.

def applyTransition (t:GaussianTransition i o) (g:Gaussian i) : Gaussian o =
  g |>
    scaleGaussian (get_at #transitionMat t) |>
    convolveGaussians (get_at #transitionNoise t)

' `kalmanGain` computes the so-called "optimal Kalman gain" corresponding to
the given input Gaussian and transition, which is used when conditioning, below.

-- TODO: avoid redundant computation (or rely on compiler CSE?)
def kalmanGain (g:Gaussian i) (t:GaussianTransition i o) : Mat i o =
  priorCov = get_at #covariance g
  h = get_at #transitionMat t
  g' = applyTransition t g
  expectedPrecision = get_at #precision g'
  priorCov ** transpose h ** expectedPrecision

' `condition` computes the conditional distribution on the input space
`i`, from the given prior and the given linear-Gaussian observation
model and observations.

def condition (g:Gaussian i) (t:GaussianTransition i o) (obs:Vec o) : Gaussian i =
  priorMean = get_at #mean g
  priorCov  = get_at #covariance g
  h = get_at #transitionMat t
  g' = applyTransition t g
  k = kalmanGain g t
  expectedMean = get_at #mean g'
  posteriorMean = priorMean + k **. (obs - expectedMean)
  r = get_at #covariance $ get_at #transitionNoise t
  tmp = eye - k ** h
  posteriorCov = tmp ** priorCov ** transpose tmp +
                 k ** r ** transpose k
  makeGaussianFromCovariance posteriorMean posteriorCov

:p
  prior = makeGaussianFromPrecision [10.0] [[2.0]]
  noise = makeGaussianFromPrecision [ 0.0] [[2.0]]
  trans = { transitionMat = eye, transitionNoise = noise }
  condition prior trans [100.0]

' ## Kalman filtering

' The `Model` record defines a linear Gaussian state space model with
latent state of type `Vec latentsIx` and observations of type `Vec
obsIx`.  The generative model is
- x.0 ~ latentsPrior
- x.(i+1) ~ transition(x.0)
- y.i ~ obsModel(x.i)

def Model (latentsIx:Type) (obsIx:Type) : Type =
 { transition : GaussianTransition latentsIx latentsIx
 & obsModel   : GaussianTransition latentsIx obsIx
 -- distribution prior to first step (before conditioning on any observation)
 & latentsPrior     : Gaussian latentsIx }

' The `kalmanFilter` function below runs a Kalman filter on a given
(dense) set of observations with respect to a given `Model`, and
computes several quantities of interest.  The results are packaged as
an array indexed by `timeIx` (the time-steps at which observations
occurred) of `LoopState`s.

' Each `LoopState` carries four relevant quantites:
- `marginalEvidence` is the log probability of the current observation
  conditioned on the model and all previous observations.
- `filteredLatents` is the filtering posterior, namely the
  distribution on this time-step's latent state conditioned on
  observations up to and including this time-step.
- `predictedLatents` is the filtered predictive distribution, namely
  the posterior on the next step's latent state conditioned on
  observations up to and including this time-step.  This is
  obtainable from `filteredLatents` by applying the dynamics once.
- `filteredObs` is the predictive distribution on observations for the
  current time-step, conditioned on observations up to but _excluding_
  the current time-step.  The `marginalEvidence` is the probability of
  the current observation under the `filteredObs` distribution.

def LoopState (latentsIx:Type) (obsIx:Type) : Type =
  { marginalEvidence : Float
  & filteredLatents  : Gaussian latentsIx
  & predictedLatents : Gaussian latentsIx
  & filteredObs : Gaussian obsIx
  }

def kalmanFilterStep
      (model:Model latentsIx obsIx)
      (obs:Vec obsIx)
      (predictedLatents:Gaussian latentsIx)
      : LoopState latentsIx obsIx =
  -- TODO: obsProb is redundant because it's computed internally within condition
  obsProb : Gaussian obsIx =
    applyTransition (get_at #obsModel model) predictedLatents
  filteredLatents : Gaussian latentsIx =
    condition predictedLatents (get_at #obsModel model) obs
  { marginalEvidence = logDensity obsProb obs
  , filteredLatents  = filteredLatents
  , predictedLatents = applyTransition (get_at #transition model) filteredLatents
  , filteredObs      = obsProb }

-- Workaround for a Dex bug with references to records
def gaussianToTup (g:Gaussian n) : (Vec n & SqMat n & SqMat n & Float) =
  ( get_at #mean g
  , get_at #covariance g
  , get_at #precision  g
  , get_at #logDetCov  g )

def tupToGaussian
      ((mean, covariance, precision, logDetCov):(Vec n & SqMat n & SqMat n & Float))
      : Gaussian n =
  { mean       = mean
  , covariance = covariance
  , precision  = precision
  , logDetCov  = logDetCov }

def kalmanFilter
      (obs : timeIx => Vec obsIx)
      (model : Model latentsIx obsIx)
      : timeIx => LoopState latentsIx obsIx =
  with_state (gaussianToTup (get_at #latentsPrior model)) \latentsRef.
    for i.
      stepResult = kalmanFilterStep model obs.i  $
                      tupToGaussian (get latentsRef)
      latentsRef := gaussianToTup $ get_at #predictedLatents stepResult
      stepResult

' ## A test model

def isotropic (scale:Float) : Gaussian n =
  makeGaussianFromCovariance zero (sq scale .* eye)

def isotropicDrift (scale:Float) : GaussianTransition n n =
  { transitionMat = eye
  , transitionNoise = isotropic scale }

random_walk : Model (Fin 1) (Fin 1) =
  { transition   = isotropicDrift 1.0
  , obsModel     = isotropicDrift 1.0
  , latentsPrior = isotropic      1.0 }

myObs = for i:(Fin 100).
  (for j:(Fin 1). i_to_f $ ordinal i)

%time
results = kalmanFilter myObs random_walk

-- :p
--   for i.
--     (getAt #mean $ getAt #filteredLatents results.i)

' ## Some real data

' From the US Census Bureau, total sales of retail and food services (excluding gas stations),
monthly from January 1992 to November 2021, in millions of dollars.  "Adjusted for
seasonal, holiday, and trading-day differences, but not for price changes".  [https://www.census.gov/retail/marts/www/adv44z72.txt](https://www.census.gov/retail/marts/www/adv44z72.txt).

-- N = 12 * 30 - 1 = 359
series : Vec (Fin 359) = [
  151292, 151612, 151082, 151999, 152742, 153191, 154291, 154708, 156154, 157321, 157645, 159630,
  161710, 160148, 158819, 163210, 164905, 164696, 167241, 167335, 168357, 169129, 170993, 172772,
  171486, 174199, 177668, 177772, 177156, 179071, 179421, 181478, 183116, 184862, 184853, 185817,
  186658, 183436, 185326, 186011, 187771, 190167, 189671, 191294, 191812, 191169, 193683, 195153,
  193220, 196551, 197940, 198424, 199811, 199716, 200257, 200292, 203168, 204700, 204361, 205232,
  206543, 208438, 209192, 208002, 206562, 209679, 212307, 212627, 213555, 213125, 213802, 214645,
  215362, 215630, 217380, 220188, 221018, 223038, 221324, 220528, 222738, 226601, 228277, 230131,
  230474, 233211, 234138, 235362, 237549, 238379, 239752, 242095, 242978, 243186, 245973, 249768,
  248686, 251806, 254373, 251139, 251152, 252537, 251725, 252344, 256221, 255515, 254123, 253667,
  256903, 257158, 255798, 259190, 258868, 258473, 258818, 260535, 254748, 274507, 267866, 264755,
  264378, 265986, 264449, 267977, 264021, 266639, 268947, 271468, 267138, 267917, 269475, 271483,
  272175, 267282, 272357, 273019, 274600, 277849, 280551, 284736, 282719, 282095, 285055, 283413,
  284365, 286137, 291233, 287994, 291953, 287908, 291804, 292425, 297408, 298084, 298487, 302828,
  300321, 304011, 304083, 306574, 305382, 314109, 315600, 309908, 307840, 308186, 314140, 314251,
  323033, 320532, 321470, 321777, 320545, 321651, 322043, 322995, 324026, 325550, 325944, 328835,
  329372, 329074, 331159, 329837, 332938, 330844, 332651, 334135, 334659, 335982, 336875, 332705,
  332813, 329329, 329701, 329639, 331692, 329908, 328225, 326639, 320952, 312366, 307574, 304092,
  308111, 305716, 300922, 302605, 303768, 306508, 307451, 312010, 303707, 306769, 307802, 308890,
  308601, 309945, 317219, 320147, 317693, 318121, 318901, 320621, 322108, 324860, 328183, 327856,
  330279, 332798, 334759, 335904, 335053, 338526, 338135, 338931, 342427, 345240, 345919, 346317,
  349968, 353158, 354120, 353208, 353523, 351939, 354083, 355771, 358448, 357723, 360574, 363140,
  366111, 368113, 366505, 366068, 368716, 369914, 372598, 372006, 372097, 374524, 375935, 376690,
  371812, 377483, 382790, 386738, 387853, 390032, 390627, 393807, 393402, 395652, 397895, 398578,
  399901, 398039, 404110, 405263, 407260, 407456, 410057, 411292, 411682, 410696, 412987, 414816,
  413296, 419477, 416628, 417329, 418410, 421785, 421253, 422753, 424361, 424817, 424829, 428661,
  433613, 432889, 432220, 434824, 433953, 436135, 436414, 435938, 443565, 444119, 448002, 449681,
  448732, 450575, 450761, 452622, 457194, 455858, 458788, 457297, 456273, 460132, 463251, 455631,
  462943, 461187, 467658, 468839, 470594, 473294, 476049, 479050, 476067, 477848, 479565, 479769,
  483735, 484185, 445730, 383688, 454679, 491896, 497521, 501511, 512312, 512059, 505232, 496874,
  535571, 517767, 576666, 582943, 573483, 577097, 565769, 572079, 574956, 584214, 584961
]

' If we plot it, we can clearly see the 2008 financial crash and
recession, and the 2020-2021 pandemic weirdness.

import plot

%time
:html show_plot $ y_plot series

' Now let's try forecasting it as a random walk and see what happens.

%time
random_walk_fit = kalmanFilter (for i. (for j. series.i)) random_walk

random_walk_fit.(from_ordinal (Fin 359) 358)

def forecastMarginals
      (model: Model latentsIx obsIx)
      (init: Gaussian latentsIx)
      : timeIx => Gaussian obsIx =
  with_state (gaussianToTup init) \latentsRef.
    for i.
      latents = tupToGaussian (get latentsRef)
      prediction = applyTransition (get_at #obsModel model) latents
      latentsRef := gaussianToTup $ applyTransition (get_at #transition model) latents
      prediction

last_fit = get_at #predictedLatents (random_walk_fit.(from_ordinal (Fin 359) 358))

:t last_fit

-- This fails for mysterious reasons
-- %time
-- forecast : (Fin 50) => Gaussian (Fin 1) = (forecastMarginals random_walk last_fit)

-- Even though this version succeeds
%time
forecast : (Fin 50) => Gaussian _ = (forecastMarginals random_walk last_fit)

:t forecast

forecast_means = for i. (get_at #mean forecast.i).(from_ordinal (Fin 1) 0)
forecast_means

%time
:html show_plot $ y_plot forecast_means

-- TODO(Issue 716) It would be nice not to have to name the variants,
-- but just have (n1|n2)=>a instead.
def array_concat (xs1: n1=>a) (xs2: n2=>a) : ({ a: n1 | b: n2 } => a) =
  for i. case i of
    {|a=i1|} -> xs1.i1
    {|b=i2|} -> xs2.i2

def yPlot2Concatenate (ys1:n1=>Float) (ys2:n2=>Float)
      : Plot ({ a: n1 | b: n2 }) Float Float Unit =
  xs = for i. i_to_f $ ordinal i
  xy_plot xs (array_concat ys1 ys2)

%time
:html show_plot $ yPlot2Concatenate series forecast_means

-- The upper 95%-ile.
def upper_p95 (marginal: Gaussian (Fin 1)) : Float =
  mean = (get_at #mean marginal).(from_ordinal _ 0)
  variance = (get_at #covariance marginal).(from_ordinal _ 0).(from_ordinal _ 0)
  scale = 1.64  -- Approximate probit(0.95)
  mean + sqrt variance * scale

forecast_p95s = for i. upper_p95 forecast.i

def yPlot2Overlay (ys1:n1=>Float) (ys2:n2=>Float)
      : Plot ({ a: n1 | b: n2 }) Float Float Unit =
  xs1 = for i:n1. i_to_f $ ordinal i
  xs2 = for i:n2. i_to_f $ ordinal i
  xy_plot (array_concat xs1 xs2) (array_concat ys1 ys2)

%time
:html show_plot $ yPlot2Overlay forecast_means forecast_p95s

def plotSeriesAndForecast
      (series: n1=>Float)
      (forecast: n2=>Gaussian _)
      : Plot ({ a: n1 | b: { a: n2 | b: n2} }) Float Float Float =
  forecast_means = for i. (get_at #mean forecast.i).(from_ordinal _ 0)
  forecast_p95s = for i. upper_p95 forecast.i
  xs1 = for i:n1. i_to_f $ ordinal i
  xs2 = for i:n2. i_to_f $ size n1 + ordinal i
  (xyc_plot (array_concat xs1 $ array_concat xs2 xs2)
            (array_concat series $ array_concat forecast_means forecast_p95s)
            $ (array_concat (for i:n1. 0.0) $ array_concat (for i:n2. 0.5) (for i:n2. 1.0)))

%time
:html show_plot $ plotSeriesAndForecast series forecast

' Predictably, the random walk forecasts that on average the series
will hold its last level.  On the scale of this series, the hard-coded
drift rate of the random walk is negligible, so we see effectively no
forecast uncertainty.

' ## A semi-local linear trend model

' Now we move to a more realistic model.  Borrowing an idea from the
structural time series package in TFP, let's try to hand-write a
semi-local linear trend model.  First we will fit this model with
guesses for the series-level parameters, just to test the Kalman
filtering on a more realistic example, and in the next section we will
learn the model parameters as well.

' As a reminder, this model assumes a latent level at each time-step,
which is growing with a speed given by a (also latent) time-varying
slope.  The slope is assumed to regress towards a global mean slope
with some global speed.  In math, we have a two-dimensional latent
state which evolves as follows:
```
level.t = level.(t-1) + slope.(t-1) + Normal(0., level_scale)
slope.t = slope_mean +
          autoregressive_coef * (slope.(t-1) - slope_mean) +
          Normal(0., slope_scale)
```
where `level_scale`, `autoregressive_coef`, `slope_mean`, and
`slope_scale` are the main parameters of the model.

' The observations are scalar, and are produced as
```
obs.t = Normal(level.t, noise_level)
```
where `noise_level` is the model's fifth scalar parameter.

-- Unit vector
def unit [Eq i] (ix: i) : Vec i =
  for dim. if dim == ix then 1.0 else 0.0

-- If you hand me a function that is actually linear, I can construct
-- a matrix out of it.
def linear_to_matrix [Eq i] (f: Vec i -> Vec o) : Mat o i =
  transpose $ for row. f (unit row)

def first (v: (Vec (Fin 2))) : _ =
  [v.(from_ordinal _ 0)]

-- But if I declare the type of `first` as
-- `first (v: (Vec (Fin 2))) : (Vec (Fin 1))`, I get the FromInteger Int32 error.
linear_to_matrix first

-- If you give me a function that is actually affine, I can construct
-- a matrix out of the linear part, and an offset vector out of the
-- affine part.
def affine_to_offset_and_matrix [Eq i] (f: Vec i -> Vec o) : (Vec o & Mat o i) =
  offset = f $ for i. 0
  (offset, linear_to_matrix (\vec. f vec - offset))

affine_to_offset_and_matrix first

-- Here the natural representation of the latent state is as a record
-- `{level: Float & slope: Float}`.  The `Model` type is, however,
-- written expecting the latent state to be a `Vec` of some index set.
-- Notionally, such an index set exists: it is the finite set
-- `{:level, :slope}`.  Is there a way to write it?  Syntax to index
-- by symbols such as `latent.level` would be very suggestive and, I
-- think, conformant with Dex's overall philosophy.
def semilocal_linear_trend
      (level_scale: Float)
      (autoregressive_coef: Float)
      (slope_mean: Float)
      (slope_scale: Float)
      (noise_level: Float)
      (latents_prior: Gaussian (Fin 2))
      : Model (Fin 2) (Fin 1) =
  level = \latent. latent.(from_ordinal _ 0)
  slope = \latent. latent.(from_ordinal _ 1)
  obsModel = { transitionMat = linear_to_matrix first
             , transitionNoise = makeGaussianFromCovariance [0] [[noise_level]]
             }
  dynamics = \latent. [ level latent + slope latent
                      , slope_mean + autoregressive_coef * (slope latent - slope_mean)
                      ]
  (offset, matrix) = affine_to_offset_and_matrix dynamics
  dynamicsModel = { transitionMat = matrix
                  , transitionNoise = makeGaussianFromCovariance offset [[level_scale, 0], [0, slope_scale]]
                  }
  { transition = dynamicsModel
  , obsModel = obsModel
  , latentsPrior = latents_prior
  }

' And now we make up some vaguely plausible parameters for it and try a fit.

init_latent_prior = makeGaussianFromCovariance [150000, 1000] [[10000 * 10000, 0], [0, 1000 * 1000]]
semilocal_candidate = semilocal_linear_trend 1000 0.98 500 100 1000 init_latent_prior

%time
semilocal_candidate_fit = kalmanFilter (for i j. series.i) semilocal_candidate

last_semilocal_candidate_fit = get_at #predictedLatents (semilocal_candidate_fit.(from_ordinal (Fin 359) 358))

%time
semilocal_forecast : (Fin 50) => _ = (forecastMarginals semilocal_candidate last_semilocal_candidate_fit)

%time
:html show_plot $ plotSeriesAndForecast series semilocal_forecast

' This model doesn't yet have much capacity for learning from the
data: the only means by which the history can affect the forecast is
by fitting the last latent state of the Kalman filter, after which the
model basically follows the parameters we slapped down for it.

' ## Learning

' Now we can try learning the parameters in this model.  The first
order of business is to try the maximum likelihood method, by gradient
ascent.  The will maximize the probability of the observed data under
this model, given the parameters.

Params : Type =
  { level_scale: Float
  & autoregressive_coef: Float
  & slope_mean: Float
  & slope_scale: Float
  & noise_level: Float
  & init_level_mean: Float
  & init_level_scale: Float
  & init_slope_mean: Float
  & init_slope_scale: Float
  }

def log_prob (params: Params) : LogProb =
  init_level_scale = get_at #init_level_scale params
  init_slope_scale = get_at #init_slope_scale params
  init_latent_prior = (makeGaussianFromCovariance
    [get_at #init_level_mean params, get_at #init_slope_mean params]
    [[init_level_scale * init_level_scale, 0],
     [0, init_slope_scale * init_slope_scale]])
  model = (semilocal_linear_trend
    (get_at #level_scale params)
    (get_at #autoregressive_coef params)
    (get_at #slope_mean params)
    (get_at #slope_scale params)
    (get_at #noise_level params)
    init_latent_prior)
  fit = kalmanFilter (for i j. series.i) model
  marginals = for i. get_at #marginalEvidence fit.i
  sum marginals

start : Params =
  { level_scale = 1000
  , autoregressive_coef = 0.98
  , slope_mean = 500
  , slope_scale = 100
  , noise_level = 1000
  , init_level_mean = 150000
  , init_level_scale = 10000
  , init_slope_mean = 1000
  , init_slope_scale = 1000
  }

%time
log_prob start

-- Compilation is too slow to be included by default.
-- %time
-- grad log_prob start
